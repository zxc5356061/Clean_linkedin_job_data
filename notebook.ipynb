{"cells":[{"source":"## Clean Job Posting data","metadata":{},"id":"0","cell_type":"markdown"},{"source":"import pandas as pd\nfrom thefuzz import process # Levenshtein algorithm\nimport matplotlib as plt","metadata":{"executionCancelledAt":null,"executionTime":813,"lastExecutedAt":1712994833054,"lastExecutedByKernel":"571aba76-ae5d-49fd-b381-ca308ecd4f00","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nfrom thefuzz import process # Levenshtein algorithm\nimport matplotlib as plt"},"cell_type":"code","id":"46e74031-0632-4d09-aeb0-0eea6a4c864a","outputs":[],"execution_count":1},{"source":"# Read csv files and drop unnecessary columns\nposting = pd.read_csv('job_postings.csv')\nposting = posting.drop(['last_processed_time','last_status','first_seen','got_summary','got_ner','is_being_worked','company','search_city','search_position'],axis=1)\n\n# print job_level requirements in different searching countries\nprint(posting.groupby(['job_level','search_country']).size())\n","metadata":{"chartConfig":{"bar":{"hasRoundedCorners":true,"stacked":false},"type":"bar","version":"v1","x":{"field":"R","type":"number"},"y":{"field":"index","type":"integer"}},"executionCancelledAt":null,"executionTime":69,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1712994833125,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Read csv files and drop unnecessary columns\nposting = pd.read_csv('job_postings.csv')\nposting = posting.drop(['last_processed_time','last_status','first_seen','got_summary','got_ner','is_being_worked','company','search_city','search_position'],axis=1)\n\n# print job_level requirements in different searching countries\nprint(posting.groupby(['job_level','search_country']).size())\n","outputsMetadata":{"0":{"height":227,"type":"stream"}},"visualizeDataframe":false,"lastExecutedByKernel":"571aba76-ae5d-49fd-b381-ca308ecd4f00","collapsed":false},"id":"1","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"job_level   search_country\nAssociate   Australia           20\n            Canada              49\n            United Kingdom     115\n            United States     1114\nMid senior  Australia          281\n            Canada             581\n            United Kingdom     880\n            United States     9177\ndtype: int64\n"}]},{"source":"# Merge job skills with job postings of junior-level positions and clean job titles\nskill = pd.read_csv('job_skills.csv')\n\ncommon_job_titles = ['Data Analyst','Data Engineer','Data Scientist','Machine Learning Engineer','Data Entry Specialist','Data Center Technician']\n\ntop_ten_associates = []\ntop_ten_seniors = []\n\nfor level in posting['job_level'].unique():\n    df = pd.merge(posting[posting['job_level']==level],skill,how='left',on='job_link')\n    for title in common_job_titles:\n        df.loc[df['job_title'].str.contains(title), 'job_title'] = title\n    \n    # Print top 10 most popular job titles\n    print(f'Top 10 most demanding job titles in {level} level \\n{df.groupby([\"job_title\"]).size().sort_values(ascending=False).head(10)}')\n    \n    # Retrieve the top four most demanding job titles\n    if level == 'Associate':\n        top_ten_associates = df.groupby(['job_title']).size().sort_values(ascending=False).iloc[0:10].index.to_list()\n    elif level == 'Mid senior':\n        top_ten_seniors = df.groupby(['job_title']).size().sort_values(ascending=False).iloc[0:10].index.to_list()\n    else: print(\"Please check syntax\")\n","metadata":{"executionCancelledAt":null,"executionTime":126,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1712994833251,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Merge job skills with job postings of junior-level positions and clean job titles\nskill = pd.read_csv('job_skills.csv')\n\ncommon_job_titles = ['Data Analyst','Data Engineer','Data Scientist','Machine Learning Engineer','Data Entry Specialist','Data Center Technician']\n\ntop_ten_associates = []\ntop_ten_seniors = []\n\nfor level in posting['job_level'].unique():\n    df = pd.merge(posting[posting['job_level']==level],skill,how='left',on='job_link')\n    for title in common_job_titles:\n        df.loc[df['job_title'].str.contains(title), 'job_title'] = title\n    \n    # Print top 10 most popular job titles\n    print(f'Top 10 most demanding job titles in {level} level \\n{df.groupby([\"job_title\"]).size().sort_values(ascending=False).head(10)}')\n    \n    # Retrieve the top four most demanding job titles\n    if level == 'Associate':\n        top_ten_associates = df.groupby(['job_title']).size().sort_values(ascending=False).iloc[0:10].index.to_list()\n    elif level == 'Mid senior':\n        top_ten_seniors = df.groupby(['job_title']).size().sort_values(ascending=False).iloc[0:10].index.to_list()\n    else: print(\"Please check syntax\")\n","outputsMetadata":{"0":{"height":428,"type":"stream"}},"collapsed":false,"lastExecutedByKernel":"571aba76-ae5d-49fd-b381-ca308ecd4f00"},"id":"2","cell_type":"code","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"Top 10 most demanding job titles in Mid senior level \njob_title\nData Engineer                                               1629\nData Analyst                                                1522\nData Scientist                                               723\nMachine Learning Engineer                                    440\nSenior MLOps Engineer                                        138\nData Architect                                               110\nManager, Data Loss Prevention (DLP) Engineer (Symantec)       57\nPrincipal Associate, Data Loss Prevention (DLP) Engineer      53\nSenior Database Administrator                                 47\nSenior MLS Engineer, Autonomous Driving Startup               45\ndtype: int64\nTop 10 most demanding job titles in Associate level \njob_title\nData Analyst                                321\nData Engineer                                96\nData Scientist                               80\nMachine Learning Engineer                    21\nData Center Technician                       20\nData Entry Specialist                        15\nData Center Engineer                         12\nVolunteer: Data and Salesforce Volunteer     11\nDatacenter Technician                        10\nVolunteer: Data Entry                         7\ndtype: int64\n"}]},{"source":"# Extract top four associate job titles with corresponding job skills\ntop_four_associates = top_ten_associates[0:4]\n\n# temp - start\ntop_list = ('Data Scientist')\ntop_df = df.loc[df['job_title'].str.contains(top_list), :]\n## temp - end\n\nskills_df = top_df['job_skills'].apply(lambda x: x.split(', ')).apply(pd.Series)\nskills_df = skills_df.fillna('')\n\n# Count existing job skills\ncols_count = {}\nfor row in [skills_df.iloc[i,:].tolist() for i in range(0,len(skills_df))]:\n    for entry in row:\n        if entry in cols_count.keys():\n            cols_count[entry] += 1\n        else:\n            cols_count[entry] = 1\n            \n\n# Store the unique values of cuisine_type in unique_types\nunique_types = cols_count.keys()\n\n# Function to clean DataFrame based on similarity scores\ndef clean_dataframe(df, threshold=80):\n    # Count occurrences of each entry\n    cols_count = df.stack().value_counts().to_dict()\n    \n    # Calculate similarity of skills\n    skills = ['Data Analysis','Data Analytics', 'Data Visualization', 'Tableau', 'Power BI', 'SQL','.Net','A/B Testing','Business Intelligence','Database Management','Data Science']\n    \n    for skill in skills:\n        matches = process.extract(skill, cols_count.keys())\n        # Iterate through the list of matches\n        for match in matches:\n            # Check whether the similarity score is greater than or equal to threshold\n            if match[1] >= threshold:\n                # Replace the matched entry in DataFrame\n                df.replace(match[0], skill, inplace=True)\n    \n    return df\n\n\n\n# Applying cleaning function to DataFrame and some manual cleaning\ntest = clean_dataframe(skills_df)\ntest = test.replace('.*AWS.*', 'AWS', regex=True)\ntest = test.replace('.*Amazon.*', 'AWS', regex=True)\ntest = test.replace('.*Agile.*', 'Agile Method', regex=True)\ntest = test.replace('.*SQL.*', 'SQL', regex=True)\ntest = test.replace('.*API.*', 'API', regex=True)\ntest = test.replace('.*Azure.*', 'Azure', regex=True)\ntest = test.replace('.*Tableau.*', 'Data Visualization', regex=True)\ntest = test.replace('.*Power BI.*', 'Data Visualization', regex=True)\ntest = test.replace('.*Business Intelligence*', 'Data Visualization', regex=True)\ntest = test.replace('.*Data Visualisation*', 'Data Visualization', regex=True)\ntest = test.replace('.*Data Integration*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Computer Science*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data validation*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data collection*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data consistency*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data integrity*', 'Data Analytics', regex=True)\ntest = test.replace('.*Data Analysis*', 'Data Analytics', regex=True)\ntest = test.replace('.*Data Exploration*', 'Data Analytics', regex=True)\ntest = test.replace('.*Extraction*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Database*', 'Database Management', regex=True)\ntest = test.replace('.*Data Engineering*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data Wrangling*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data Pipelines*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data Security*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data Modeling*', 'ETL/ELT', regex=True)\ntset = test.replace('.*Data Management*', 'ETL/ELT', regex=True)\ntest = test.replace('.*python*', 'Python', regex=True)\n\n# To count cleaned skills again\ncount_clean = {}\nfor row in [test.iloc[i,:].tolist() for i in range(0,len(test))]:\n    for entry in row:\n        if entry in count_clean.keys():\n            count_clean[entry] += 1\n        else:\n            count_clean[entry] = 1\n\n            \nclean_df = pd.DataFrame.from_dict(count_clean,orient='index')\nclean_df = clean_df.reset_index()\nclean_df.rename(columns={'index':'skill',0:'counts'},inplace = True)\n# To exclude empty strings\nclean_df_sorted = clean_df[clean_df['skill'] != ''].sort_values(by='counts', ascending=False)\n\nprint(clean_df_sorted.head(20))","metadata":{"executionCancelledAt":null,"executionTime":379,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1712994879938,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Extract top four associate job titles with corresponding job skills\ntop_four_associates = top_ten_associates[0:4]\n\n# temp - start\ntop_list = ('Data Scientist')\ntop_df = df.loc[df['job_title'].str.contains(top_list), :]\n## temp - end\n\nskills_df = top_df['job_skills'].apply(lambda x: x.split(', ')).apply(pd.Series)\nskills_df = skills_df.fillna('')\n\n# Count existing job skills\ncols_count = {}\nfor row in [skills_df.iloc[i,:].tolist() for i in range(0,len(skills_df))]:\n    for entry in row:\n        if entry in cols_count.keys():\n            cols_count[entry] += 1\n        else:\n            cols_count[entry] = 1\n            \n\n# Store the unique values of cuisine_type in unique_types\nunique_types = cols_count.keys()\n\n# Function to clean DataFrame based on similarity scores\ndef clean_dataframe(df, threshold=80):\n    # Count occurrences of each entry\n    cols_count = df.stack().value_counts().to_dict()\n    \n    # Calculate similarity of skills\n    skills = ['Data Analysis','Data Analytics', 'Data Visualization', 'Tableau', 'Power BI', 'SQL','.Net','A/B Testing','Business Intelligence','Database Management','Data Science']\n    \n    for skill in skills:\n        matches = process.extract(skill, cols_count.keys())\n        # Iterate through the list of matches\n        for match in matches:\n            # Check whether the similarity score is greater than or equal to threshold\n            if match[1] >= threshold:\n                # Replace the matched entry in DataFrame\n                df.replace(match[0], skill, inplace=True)\n    \n    return df\n\n\n\n# Applying cleaning function to DataFrame and some manual cleaning\ntest = clean_dataframe(skills_df)\ntest = test.replace('.*AWS.*', 'AWS', regex=True)\ntest = test.replace('.*Amazon.*', 'AWS', regex=True)\ntest = test.replace('.*Agile.*', 'Agile Method', regex=True)\ntest = test.replace('.*SQL.*', 'SQL', regex=True)\ntest = test.replace('.*API.*', 'API', regex=True)\ntest = test.replace('.*Azure.*', 'Azure', regex=True)\ntest = test.replace('.*Tableau.*', 'Data Visualization', regex=True)\ntest = test.replace('.*Power BI.*', 'Data Visualization', regex=True)\ntest = test.replace('.*Business Intelligence*', 'Data Visualization', regex=True)\ntest = test.replace('.*Data Visualisation*', 'Data Visualization', regex=True)\ntest = test.replace('.*Data Integration*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Computer Science*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data validation*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data collection*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data consistency*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data integrity*', 'Data Analytics', regex=True)\ntest = test.replace('.*Data Analysis*', 'Data Analytics', regex=True)\ntest = test.replace('.*Data Exploration*', 'Data Analytics', regex=True)\ntest = test.replace('.*Extraction*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Database*', 'Database Management', regex=True)\ntest = test.replace('.*Data Engineering*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data Wrangling*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data Pipelines*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data Security*', 'ETL/ELT', regex=True)\ntest = test.replace('.*Data Modeling*', 'ETL/ELT', regex=True)\ntset = test.replace('.*Data Management*', 'ETL/ELT', regex=True)\ntest = test.replace('.*python*', 'Python', regex=True)\n\n# To count cleaned skills again\ncount_clean = {}\nfor row in [test.iloc[i,:].tolist() for i in range(0,len(test))]:\n    for entry in row:\n        if entry in count_clean.keys():\n            count_clean[entry] += 1\n        else:\n            count_clean[entry] = 1\n\n            \nclean_df = pd.DataFrame.from_dict(count_clean,orient='index')\nclean_df = clean_df.reset_index()\nclean_df.rename(columns={'index':'skill',0:'counts'},inplace = True)\n# To exclude empty strings\nclean_df_sorted = clean_df[clean_df['skill'] != ''].sort_values(by='counts', ascending=False)\n\nprint(clean_df_sorted.head(20))","outputsMetadata":{"0":{"height":428,"type":"stream"}},"collapsed":false,"lastExecutedByKernel":"571aba76-ae5d-49fd-b381-ca308ecd4f00"},"id":"3","cell_type":"code","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"                       skill  counts\n93        Data Visualization      75\n31                    Python      65\n61              Data Science      61\n34                       SQL      56\n109           Data Analytics      55\n65          Machine Learning      45\n90                   ETL/ELT      45\n13                         R      38\n5                 Statistics      26\n243            Communication      20\n75                     Spark      18\n76                    Hadoop      18\n0           Machine learning      17\n41                       AWS      17\n203              Data Mining      16\n113               Clustering      12\n42                      Java      12\n1                 Regression      11\n140  Artificial Intelligence      10\n193               Leadership      10\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}